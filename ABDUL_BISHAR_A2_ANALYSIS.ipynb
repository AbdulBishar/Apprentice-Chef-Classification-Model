{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student Name : Abdul Bishar\n",
    "# Cohort       : 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Chef Inc:Case Challenge Part II(Individual Assignment 2)</h1>\n",
    "\n",
    "In an effort to diversify their revenue stream, Apprentice Chef, Inc. has launched Halfway There, a cross-selling promotion where subscribers receive a half bottle of wine from a local California vineyard every Wednesday (halfway through the work week).The executives at Apprentice Chef also believe this endeavor will create a competitive advantage based on its unique product offering of hard to find local wines.Halfway There has been exclusively offered to all of the customers in the dataset you received, and the executives would like to promote this service to a wider audience.They have tasked you with analyzing their data, developing your top insights, and building a machine learning model to predict which customers will subscribe to this service.\n",
    "\n",
    "\n",
    "Tasks: \n",
    "\n",
    "1) Analyze Apprentice Chef's customer user data\n",
    "\n",
    "2) Develop insights about the \"Halfway There promotion\"\n",
    "\n",
    "3) Build a model to predict cross sell success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Preperation</h1>\n",
    "\n",
    "<h2>Loading packages</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdul\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import Packages\n",
    "\n",
    "import pandas as pd                                                   # data science essentials\n",
    "import numpy as np                                                    # Numeric Python\n",
    "import matplotlib.pyplot as plt                                       # data viz\n",
    "import seaborn as sns                                                 # Enhance data viz   \n",
    "\n",
    "\n",
    "import statsmodels.formula.api as smf                                 # stats model for regression\n",
    "from sklearn.model_selection import train_test_split,cross_val_score  # Train Tests Split data \n",
    "from sklearn.neighbors import KNeighborsClassifier                    # KNN for Classification\n",
    "from sklearn.preprocessing import StandardScaler                      # Standard scaler\n",
    "from sklearn.linear_model import LogisticRegression                   # Logistic Regression\n",
    "from sklearn.metrics import roc_auc_score                             # AUC curve\n",
    "from sklearn.ensemble import GradientBoostingClassifier               # Gradient Boosting Regressor\n",
    "from sklearn.model_selection import GridSearchCV                      # GridSearch\n",
    "from sklearn.metrics import confusion_matrix                          # Confusion Matrix\n",
    "from sklearn.metrics import make_scorer                               # customizable scorer\n",
    " \n",
    "# libraries for classification trees\n",
    "from sklearn.tree import DecisionTreeClassifier                       # classification trees\n",
    "from sklearn.ensemble import GradientBoostingClassifier               # Gradient Boosting Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier                   # Random Forest for classification\n",
    "from sklearn.tree import export_graphviz                              # exports graphics\n",
    "from sklearn.externals.six import StringIO                            # saves objects in memory\n",
    "from IPython.display import Image                                     # displays on frontend\n",
    "from sklearn.tree import export_graphviz                              # exports graphics\n",
    "import pydotplus                                                      # interprets dot objects "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data dictionary</h2>\n",
    "\n",
    "Contains description of features and can be useful as a reference throughout the analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>REVENUE</td>\n",
       "      <td>float</td>\n",
       "      <td>Total revenue generated over the first year of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>CROSS_SELL_SUCCESS</td>\n",
       "      <td>integer</td>\n",
       "      <td>Success of promoting Halfway There (1 = SUCCES...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>NAME</td>\n",
       "      <td>string</td>\n",
       "      <td>Full name of customer (collected upon initial ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>EMAIL</td>\n",
       "      <td>string</td>\n",
       "      <td>Email of customer (collected upon initial regi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>FIRST_NAME</td>\n",
       "      <td>string</td>\n",
       "      <td>First name of customer (collected upon initial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>FAMILY_NAME</td>\n",
       "      <td>string</td>\n",
       "      <td>Last name of customer (collected upon initial ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>TOTAL_MEALS_ORDERED</td>\n",
       "      <td>integer</td>\n",
       "      <td>Total count of meals ordered per customer account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>UNIQUE_MEALS_PURCH</td>\n",
       "      <td>integer</td>\n",
       "      <td>Count of unique meal sets ordered per customer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>CONTACTS_W_CUSTOMER_SERVICE</td>\n",
       "      <td>integer</td>\n",
       "      <td>Count of times a customer made contact with cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>PRODUCT_CATEGORIES_VIEWED</td>\n",
       "      <td>integer</td>\n",
       "      <td>Total number of meal categories viewed (online...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>AVG_TIME_PER_SITE_VISIT</td>\n",
       "      <td>float</td>\n",
       "      <td>Average platform (web or mobile) visit time pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>MOBILE_NUMBER</td>\n",
       "      <td>integer</td>\n",
       "      <td>Customer registered with a mobile or landline ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>CANCELLATIONS_BEFORE_NOON</td>\n",
       "      <td>integer</td>\n",
       "      <td>Number of meals canceled before 12 PM as per c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>CANCELLATIONS_AFTER_NOON</td>\n",
       "      <td>integer</td>\n",
       "      <td>Number of meals canceled after 3 PM as per can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>TASTES_AND_PREFERENCES</td>\n",
       "      <td>integer</td>\n",
       "      <td>Customer specified their tastes and preference...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>MOBILE_LOGINS</td>\n",
       "      <td>integer</td>\n",
       "      <td>Count of logins on the mobile platform (app)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>PC_LOGINS</td>\n",
       "      <td>integer</td>\n",
       "      <td>Count of logins on the web platform (website)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>WEEKLY_PLAN</td>\n",
       "      <td>integer</td>\n",
       "      <td>Count of weeks a customer subscribed to the we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>EARLY_DELIVERIES</td>\n",
       "      <td>integer</td>\n",
       "      <td>Count of orders that we delivered BEFORE the a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>LATE_DELIVERIES</td>\n",
       "      <td>integer</td>\n",
       "      <td>Count of orders that we delivered AFTER the al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>PACKAGE_LOCKER</td>\n",
       "      <td>integer</td>\n",
       "      <td>Customer's building has a package locker servi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>REFRIGERATED_LOCKER</td>\n",
       "      <td>integer</td>\n",
       "      <td>Package room has a refrigerated locker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>FOLLOWED_RECOMMENDATIONS_PCT</td>\n",
       "      <td>float</td>\n",
       "      <td>Percentage of time a customer followed meal re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>AVG_PREP_VID_TIME</td>\n",
       "      <td>float</td>\n",
       "      <td>Average time in seconds a customer watched  in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>LARGEST_ORDER_SIZE</td>\n",
       "      <td>integer</td>\n",
       "      <td>Largest number of meals a customer has ordered...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>MASTER_CLASSES_ATTENDED</td>\n",
       "      <td>integer</td>\n",
       "      <td>Count of times a customer attended master clas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>MEDIAN_MEAL_RATING</td>\n",
       "      <td>integer</td>\n",
       "      <td>Median meal satisfaction rating by customer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>AVG_CLICKS_PER_VISIT</td>\n",
       "      <td>float</td>\n",
       "      <td>Average number of clicks per site visit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>TOTAL_PHOTOS_VIEWED</td>\n",
       "      <td>integer</td>\n",
       "      <td>Count of photos viewed on web and mobile platf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Column Data Type  \\\n",
       "0                        REVENUE     float   \n",
       "1             CROSS_SELL_SUCCESS   integer   \n",
       "2                           NAME    string   \n",
       "3                          EMAIL    string   \n",
       "4                     FIRST_NAME    string   \n",
       "5                    FAMILY_NAME    string   \n",
       "6            TOTAL_MEALS_ORDERED   integer   \n",
       "7             UNIQUE_MEALS_PURCH   integer   \n",
       "8    CONTACTS_W_CUSTOMER_SERVICE   integer   \n",
       "9      PRODUCT_CATEGORIES_VIEWED   integer   \n",
       "10       AVG_TIME_PER_SITE_VISIT     float   \n",
       "11                 MOBILE_NUMBER   integer   \n",
       "12     CANCELLATIONS_BEFORE_NOON   integer   \n",
       "13      CANCELLATIONS_AFTER_NOON   integer   \n",
       "14        TASTES_AND_PREFERENCES   integer   \n",
       "15                 MOBILE_LOGINS   integer   \n",
       "16                     PC_LOGINS   integer   \n",
       "17                   WEEKLY_PLAN   integer   \n",
       "18              EARLY_DELIVERIES   integer   \n",
       "19               LATE_DELIVERIES   integer   \n",
       "20                PACKAGE_LOCKER   integer   \n",
       "21           REFRIGERATED_LOCKER   integer   \n",
       "22  FOLLOWED_RECOMMENDATIONS_PCT     float   \n",
       "23             AVG_PREP_VID_TIME     float   \n",
       "24            LARGEST_ORDER_SIZE   integer   \n",
       "25       MASTER_CLASSES_ATTENDED   integer   \n",
       "26            MEDIAN_MEAL_RATING   integer   \n",
       "27          AVG_CLICKS_PER_VISIT     float   \n",
       "28           TOTAL_PHOTOS_VIEWED   integer   \n",
       "\n",
       "                                          Description  \n",
       "0   Total revenue generated over the first year of...  \n",
       "1   Success of promoting Halfway There (1 = SUCCES...  \n",
       "2   Full name of customer (collected upon initial ...  \n",
       "3   Email of customer (collected upon initial regi...  \n",
       "4   First name of customer (collected upon initial...  \n",
       "5   Last name of customer (collected upon initial ...  \n",
       "6   Total count of meals ordered per customer account  \n",
       "7   Count of unique meal sets ordered per customer...  \n",
       "8   Count of times a customer made contact with cu...  \n",
       "9   Total number of meal categories viewed (online...  \n",
       "10  Average platform (web or mobile) visit time pe...  \n",
       "11  Customer registered with a mobile or landline ...  \n",
       "12  Number of meals canceled before 12 PM as per c...  \n",
       "13  Number of meals canceled after 3 PM as per can...  \n",
       "14  Customer specified their tastes and preference...  \n",
       "15       Count of logins on the mobile platform (app)  \n",
       "16      Count of logins on the web platform (website)  \n",
       "17  Count of weeks a customer subscribed to the we...  \n",
       "18  Count of orders that we delivered BEFORE the a...  \n",
       "19  Count of orders that we delivered AFTER the al...  \n",
       "20  Customer's building has a package locker servi...  \n",
       "21             Package room has a refrigerated locker  \n",
       "22  Percentage of time a customer followed meal re...  \n",
       "23  Average time in seconds a customer watched  in...  \n",
       "24  Largest number of meals a customer has ordered...  \n",
       "25  Count of times a customer attended master clas...  \n",
       "26        Median meal satisfaction rating by customer  \n",
       "27            Average number of clicks per site visit  \n",
       "28  Count of photos viewed on web and mobile platf...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#reading in data dictionary \n",
    "chef_description = pd.read_excel('Apprentice_Chef_Data_Dictionary.xlsx')\n",
    "\n",
    "#displaying the data dictionary\n",
    "chef_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Dataset inital exploration </h2>\n",
    "\n",
    "\n",
    "68% of the customers in this data set have already taken advantage of the \"Halfway there\" promotion\n",
    "\n",
    "Also noteworthy were 47 missing values for FAMILY_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in dataset and printing first 5 rows\n",
    "file = 'Apprentice_Chef_Dataset.xlsx'\n",
    "\n",
    "original = pd.read_excel(file)\n",
    "\n",
    "#copy of original dataset\n",
    "chef = original.copy()\n",
    "\n",
    "#chef.head(n=5)\n",
    "\n",
    "# Exploring the data and structure\n",
    "#chef.info()\n",
    "#chef.describe()\n",
    "\n",
    "# Exploring the original variables in the dataset\n",
    "# chef.columns\n",
    "# chef.isnull().sum()\n",
    "\n",
    "\n",
    "customers_used_cross_sell = chef['CROSS_SELL_SUCCESS'].sum()\n",
    "total_rows = chef['CROSS_SELL_SUCCESS'].count()\n",
    "\n",
    "cross_sell_percentage = (customers_used_cross_sell/total_rows)*100\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Feature Enigneering</h1>\n",
    "\n",
    "<h2> Missing value treatment </h2>\n",
    "\n",
    "The  47 missing values for 'FAMILY_NAME' variable represent  <strong> only 2.4% of observations</strong> with missing values. Therefore it is deemed unnecessary for any valuable or meaningful imputation and left as \"Uknown\". \n",
    "\n",
    "Moreover the the FAMILY_NAME was later <strong> found to be insignificant </strong>in determining whether someone will subscribe to Halfway there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#47 missing values identified for FAMILY_NAME \n",
    "fill = 'Unknown'\n",
    "chef['FAMILY_NAME'] = chef['FAMILY_NAME'].fillna(fill)            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> New Feature: Average price per meal </h2>\n",
    "\n",
    "A new feature \"AVERAGE_PRICE_PER_MEAl\" is added (including weekly subscription)\n",
    "\n",
    "My hypothesis is that people who spend higher on average may be more likely to participate in the promotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Avg. Price per Meal variable\n",
    "chef['AVG_PRICE_PER_MEAL'] = chef['REVENUE']/chef['TOTAL_MEALS_ORDERED']\n",
    "\n",
    "#chef['AVG_PRICE_PER_MEAL'].quantile([0.10,\n",
    "                                      #0.25,\n",
    "                                      #0.50,\n",
    "                                      #0.75,\n",
    "                                      #0.90])\n",
    "                \n",
    "#47 missing values identified for FAMILY_NAME \n",
    "fill = 'Unknown'\n",
    "chef['FAMILY_NAME'] = chef['FAMILY_NAME'].fillna(fill)            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> New Features: Email </h2>\n",
    "\n",
    "This next cell focuses on splitting emails into 3 seperate categories, peronal,professional and junk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummie variables from the email domain.\n",
    "# Converting dataset to a dataFrame in order to use .iterrows()\n",
    "chef_email       = pd.DataFrame(chef['EMAIL'])\n",
    "\n",
    "placeholder_lst  = []\n",
    "\n",
    "for index, col in chef_email.iterrows():\n",
    "    split_email  = chef_email.loc[index, 'EMAIL'].split(sep = '@')\n",
    "    \n",
    "    placeholder_lst.append(split_email)\n",
    "    \n",
    "email_df         = pd.DataFrame(placeholder_lst)\n",
    "email_df.columns = ['name', 'domain']\n",
    "\n",
    "\n",
    "# Domain groups\n",
    "personal_email     = ['@gmail.com', '@yahoo.com','@protonmail.com']\n",
    "professional_email = ['@mmm.com', '@amex.com','@apple.com',\n",
    "                      '@boeing.com','@caterpillar.com',\n",
    "                      '@chevron.com','@cisco.com','@cocacola.com',\n",
    "                      '@disney.com','@dupont.com','@exxon.com',\n",
    "                      '@ge.org','@goldmansacs.com','@homedepot.com',\n",
    "                      '@ibm.com','@intel.com','@jnj.com',\n",
    "                      '@jpmorgan.com','@mcdonalds.com','@merck.com',\n",
    "                      '@microsoft.com','@nike.com','@pfizer.com',\n",
    "                      '@pg.com','@travelers.com','@unitedtech.com',\n",
    "                      '@unitedhealth.com','@verizon.com','@visa.com',\n",
    "                      '@walmart.com']\n",
    "junk_email          = ['@me.com', '@aol.com', '@hotmail.com', '@live.com',\n",
    "                       '@msn.com','@passport.com']\n",
    "\n",
    "\n",
    "# For loop categorising the different email domains\n",
    "placeholder_lst = []\n",
    "\n",
    "for domain in email_df['domain']:\n",
    "    \n",
    "    if '@' + domain in personal_email:\n",
    "        placeholder_lst.append('personal')\n",
    "    elif '@' + domain in professional_email:\n",
    "        placeholder_lst.append('professional')\n",
    "    else:\n",
    "        placeholder_lst.append('junk')\n",
    "        \n",
    "# make the columns into a series to append it to original dataset        \n",
    "email_df['DOMAIN_GROUP'] = pd.Series(placeholder_lst)\n",
    "\n",
    "# Concatenate the email domains as a new column in the chef DataFrame \n",
    "chef['DOMAIN'] = email_df['DOMAIN_GROUP']\n",
    "\n",
    "# Get dummies from the domain variable and drop the original column\n",
    "one_hot_DOMAIN = pd.get_dummies(chef['DOMAIN'])\n",
    "\n",
    "# Remove the old and add the 3 new columns\n",
    "chef           = chef.join([one_hot_DOMAIN])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> New Feature: Valid emails </h2>\n",
    "\n",
    "Then next cell creates a binary feature that groups personal and professional emails into one group, Valid emails. Valid emails given a value of 1 and anything else i.e. junk emails are given a value of 0\n",
    "\n",
    "It was found that <strong> 80% of emails are considered valid and 20% are junk </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variable to gauge intent by seeing if email is Valid\n",
    "\n",
    "placeholder_lst = []\n",
    "\n",
    "for row,col in chef.iterrows():\n",
    "    if chef.loc[row,'DOMAIN'] == 'junk':\n",
    "        placeholder_lst.append(0)\n",
    "    else:\n",
    "        placeholder_lst.append(1)\n",
    "\n",
    "# Adding the new variable to the original dataset\n",
    "chef['VALID_EMAIL'] = pd.Series(placeholder_lst)\n",
    "\n",
    "#chef['VALID_EMAIL'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> New Features: Numbers of Names </h2>\n",
    "\n",
    "This next feature developed counts the number of names a user has. My hypothesis is that perhaps people with longer titles may have more royalty and have larger spending power which could affect the promotion success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding variable, counting the number of names in NAME column \n",
    "\n",
    "def text_split_feature(col, df, sep=' ', new_col_name=None):\n",
    "    \"\"\"\n",
    "Splits values in a string Series (as part of a DataFrame) and sums the number\n",
    "of resulting items. Automatically appends summed column to original DataFrame.\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "col          : column to split\n",
    "df           : DataFrame where column is located\n",
    "sep          : string sequence to split by, default ' '\n",
    "new_col_name : name of new column after summing split, default\n",
    "               'number_of_names'\n",
    "\"\"\"\n",
    "    \n",
    "    chef[new_col_name] = 0\n",
    "    \n",
    "    \n",
    "    for index, val in chef.iterrows():\n",
    "        chef.loc[index, new_col_name] = len(chef.loc[index, col].split(sep = ' '))\n",
    "        \n",
    "text_split_feature(col = 'NAME', df = chef, new_col_name = 'NUMBER_NAMES' )                                                                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> New Feature: Master class attended </h2>\n",
    "\n",
    "This next feature is a binary feature that determines whether someone has ever attended a master class before. My hypothesis is that people who are engaged enough to come for a master class may be more likely to participate in the promotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making attending a master class into a binary variable (1= attended master class, 0 = never attended masterclass)\n",
    "\n",
    "#empty list to be appended at the end\n",
    "placeholder_lst = []\n",
    "\n",
    "#for loop to loop over master classes attended and apply binary variables\n",
    "\n",
    "for row,col in chef.iterrows():\n",
    "    if chef.loc[row,'MASTER_CLASSES_ATTENDED'] >= 1:\n",
    "        placeholder_lst.append(1)\n",
    "    else:\n",
    "        placeholder_lst.append(0)\n",
    "\n",
    "# Adding the new variable to the original dataset\n",
    "chef['ATTENDED_MASTER_CLASS'] = pd.Series(placeholder_lst)\n",
    "#chef = chef.drop(columns = 'MASTER_CLASSES_ATTENDED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> New Feature: Noblity </h2>\n",
    "\n",
    "This next feature was developed from the idea from feature 3.4. This feature looks for certain noble titles in the name column and creates a binary feature for whether a user is noble or not. My hypothesis is that nobles may have more money to spend and are better targets for the promotion\n",
    "\n",
    "Library of titles are available at <a href = \"https://en.wikipedia.org/wiki/Imperial,_royal_and_noble_ranks\">This Wikipedia page</a>\n",
    "\n",
    "By doing addiditional research into <a href = \"https://awoiaf.westeros.org/index.php/Hizdahr_zo_Loraq\"> The city of westoros</a> it was discovered that titles like \"mo\" and \"zo\" were refences to \"of\" and having a reference of \"of\" usually refers to \"daughter of...\" or \"Son of...\" and thus should also be labelled a Noble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating variable that determines whether individual is NOBLE\n",
    "\n",
    "#empty list to be appended at the end\n",
    "placeholder_lst = []\n",
    "\n",
    "#for loop to loop over names and match noble titles\n",
    "\n",
    "for row,pattern in chef.iterrows():\n",
    "    if ' of ' in chef.loc[row,'NAME'] or \\\n",
    "    'lord' in chef.loc[row,'NAME'] or \\\n",
    "    'Lord' in chef.loc[row,'NAME'] or \\\n",
    "    ' mo ' in chef.loc[row,'NAME'] or \\\n",
    "    ' zo ' in chef.loc[row,'NAME'] or \\\n",
    "    ' Mo ' in chef.loc[row,'NAME'] or \\\n",
    "    'Knight' in chef.loc[row, 'NAME'] or \\\n",
    "    'knight'in chef.loc[row, 'NAME']:\n",
    "        placeholder_lst.append(1)\n",
    "    else:\n",
    "        placeholder_lst.append(0)\n",
    "        \n",
    "\n",
    "#creating column by appending empty list\n",
    "chef['NOBLE'] = pd.Series(placeholder_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> New Feature: Willing to try new products </h2>\n",
    "\n",
    "This next feature is a binary feature that classifies someone as \"usually\" willing to try new products if more than 20% of the time, they order a meal they've never had before. My hypothesis is that these individuals are more experimental and may be more likely to try out the new cross sell promotion\n",
    "\n",
    "It was found that <strong> 14% of customers fall under this \"usually\" willing to try new products group</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Creating a column for % of unique meals purchased\n",
    "chef['PERCENT_UNIQUE_MEALS']= round(chef['UNIQUE_MEALS_PURCH']/ chef['TOTAL_MEALS_ORDERED']*100, 2)\n",
    "\n",
    "placeholder_lst = []\n",
    "\n",
    "\n",
    "for row,col in chef.iterrows():\n",
    "    if chef.loc[row,'PERCENT_UNIQUE_MEALS'] >= 20:\n",
    "        placeholder_lst.append(1)\n",
    "    else:\n",
    "        placeholder_lst.append(0)\n",
    "\n",
    "# Adding the new variable to the original dataset\n",
    "chef['WILLIGNESS_NEW_PRODUCTS'] = pd.Series(placeholder_lst)\n",
    "\n",
    "#chef['WILLIGNESS_NEW_PRODUCTS'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> New Feature: \"Normally\" follows recommendations </h2>\n",
    "\n",
    "This next feature is a binary feature that classifies whether people \"normally\" follow recommendations by determining if they follow recommendations   <strong>More than half of the time </strong>\n",
    "\n",
    "It was found that <strong> 35% of users fall into this \"Normally\" follows recommendation group </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow recomendations appears to be significant. Create a column based on this \n",
    "\n",
    "placeholder_lst = []\n",
    "\n",
    "\n",
    "for row,col in chef.iterrows():\n",
    "    if chef.loc[row,'FOLLOWED_RECOMMENDATIONS_PCT'] >= 50:\n",
    "        placeholder_lst.append(1)\n",
    "    else:\n",
    "        placeholder_lst.append(0)\n",
    "\n",
    "# Adding the new variable to the original dataset\n",
    "chef['NORMALLY_FOLLOWS_RECOMMENDATIONS'] = pd.Series(placeholder_lst)\n",
    "\n",
    "#chef['NORMALLY_FOLLOWS_RECOMMENDATIONS'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> New Feature: Ordered Beverages Before </h2>\n",
    "\n",
    "This next feature is a binary feature that classifies whether people on average, order a drink with their food or not. The highest possible \"Meal Only\" price is 23 dollars, the only way that someones meal price can be higher than 23 dollars is if they order a drink. My hypothesis is that people who also buy drinks are more likely to participate in the promotion\n",
    "\n",
    "It was found that 27% of users fall into this group\n",
    "\n",
    "<strong>Assumption: </strong> It is definitely possible that people order an extra drink that is not wine. This feature assumes all extra drinks are wine\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating ordered beverage column\n",
    "chef['ORDERED_BEVERAGES'] = 0\n",
    "\n",
    "# looping to find Miss.\n",
    "for index, val in chef.iterrows():\n",
    "    \n",
    "    if chef.loc[index, 'AVG_PRICE_PER_MEAL'] > 23:\n",
    "        chef.loc[index, 'ORDERED_BEVERAGES'] = 1\n",
    "        \n",
    "    elif chef.loc[index,'AVG_PRICE_PER_MEAL'] <= 23:\n",
    "        chef.loc[index, 'ORDERED_BEVERAGES'] = 0\n",
    "    \n",
    "    else:\n",
    "        print('error')\n",
    "        \n",
    "        \n",
    "#chef['ORDERED_BEVERAGES'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Outliers and thresholds </h2>\n",
    "\n",
    "Next we look at a visual representation of each feature to study the distribution of values and we will use the \"eyeball\" method to determine high and low outliers for certain variables\n",
    "\n",
    "Histograms and Boxplots are available to produce in the next two cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#creating ONLY numerical version of original dataset by dropping charecter features\n",
    "numerical_chef = chef.drop(columns = ['NAME', 'EMAIL', 'FIRST_NAME', 'FAMILY_NAME','DOMAIN'])\n",
    "\n",
    "#double checking that all non numerical objects removed\n",
    "#numerical_chef.info()\n",
    "\n",
    "#for loop to print historgram of all columns\n",
    "#for col in numerical_chef:\n",
    "    \n",
    "    #fig, ax = plt.subplots(figsize = (10, 8))\n",
    "    \n",
    "    #plt.hist(numerical_chef[col], bins = 100)\n",
    "    #xlabel = print(f'{col}')\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for loop to print boxplot of all columns\n",
    "#for col in numerical_chef:\n",
    "    \n",
    "    #fig, ax = plt.subplots(figsize = (8, 6))\n",
    "    \n",
    "    #plt.boxplot(x = numerical_chef[col],\n",
    "                #data = numerical_chef)\n",
    "    #xlabel = print(f'{col}')\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Outlier features </h3>\n",
    "\n",
    "Next we create an outlier feature for every feature where we decided to mark outliers for \n",
    "\n",
    "NB: If a feature has a high and low outliers they will be grouped together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers thresholds determined based on the histograms and scatterplots\n",
    "revenue_hi                    = 6200\n",
    "total_meals_hi                = 230\n",
    "unique_meals_hi               = 10\n",
    "contact_w_customer_service_hi = 12\n",
    "avg_time_per_site_hi          = 250\n",
    "cancel_before_noon_hi         = 7\n",
    "late_deliveries_hi            = 15\n",
    "early_delivery_lo             = 0\n",
    "total_photos_viewed_lo        = 0\n",
    "follow_recommendations_pct_hi = 40\n",
    "pc_log_hi                     = 7\n",
    "pc_log_lo                     = 3\n",
    "mobile_log_hi                 = 3\n",
    "mobile_log_lo                 = 0\n",
    "weekly_plan_hi                = 15\n",
    "avg_prep_video_hi             = 300\n",
    "total_photos_viewed_lo        = 0\n",
    "percent_unique_meals_hi       = 30\n",
    "avg_meal_price_hi             = 120\n",
    "follow_recommendations_pct_lo = 10\n",
    "\n",
    "\n",
    "# REVENUE\n",
    "chef['out_REVENUE']  = 0\n",
    "condition_hi = chef.loc[0:,'out_REVENUE'][chef['REVENUE'] \n",
    "                                                          > revenue_hi]\n",
    "\n",
    "chef['out_REVENUE'].replace(to_replace = condition_hi,\n",
    "                                   value      = 1,\n",
    "                                   inplace    = True)\n",
    "# TOTAL_MEALS_ORDERED\n",
    "chef['out_TOTAL_MEALS_ORDERED']  = 0\n",
    "condition_hi = chef.loc[0:,'out_TOTAL_MEALS_ORDERED'][chef['TOTAL_MEALS_ORDERED'] \n",
    "                                                             > total_meals_hi]\n",
    "\n",
    "chef['out_TOTAL_MEALS_ORDERED'].replace(to_replace = condition_hi,\n",
    "                                               value      = 1,\n",
    "                                               inplace    = True)\n",
    "\n",
    "# UNIQUE_MEALS_PURCH\n",
    "chef['out_UNIQUE_MEALS_PURCH']  = 0\n",
    "condition_hi = chef.loc[0:,'out_UNIQUE_MEALS_PURCH'][chef['UNIQUE_MEALS_PURCH'] \n",
    "                                                            > unique_meals_hi]\n",
    "\n",
    "chef['out_UNIQUE_MEALS_PURCH'].replace(to_replace = condition_hi,\n",
    "                                              value      = 1,\n",
    "                                              inplace    = True)\n",
    "\n",
    "# CONTACTS_W_CUSTOMER_SERVICE\n",
    "chef['out_CONTACTS_W_CUSTOMER_SERVICE']  = 0\n",
    "condition_hi = chef.loc[0:,'out_CONTACTS_W_CUSTOMER_SERVICE'][chef['CONTACTS_W_CUSTOMER_SERVICE'] \n",
    "                                                                     > contact_w_customer_service_hi]\n",
    "\n",
    "chef['out_CONTACTS_W_CUSTOMER_SERVICE'].replace(to_replace = condition_hi,\n",
    "                                                       value      = 1,\n",
    "                                                       inplace    = True)\n",
    "\n",
    "# AVG_TIME_PER_SITE_VISIT\n",
    "chef['out_AVG_TIME_PER_SITE_VISIT']  = 0\n",
    "condition_hi = chef.loc[0:,'out_AVG_TIME_PER_SITE_VISIT'][chef['AVG_TIME_PER_SITE_VISIT'] \n",
    "                                                                 > avg_time_per_site_hi]\n",
    "\n",
    "chef['out_AVG_TIME_PER_SITE_VISIT'].replace(to_replace = condition_hi,\n",
    "                                                   value      = 1,\n",
    "                                                   inplace    = True)\n",
    "\n",
    "# CANCELLATIONS_BEFORE_NOON\n",
    "chef['out_CANCELLATIONS_BEFORE_NOON']  = 0\n",
    "condition_hi = chef.loc[0:,'out_CANCELLATIONS_BEFORE_NOON'][chef['CANCELLATIONS_BEFORE_NOON'] \n",
    "                                                                   > cancel_before_noon_hi]\n",
    "\n",
    "chef['out_CANCELLATIONS_BEFORE_NOON'].replace(to_replace = condition_hi,\n",
    "                                                     value      = 1,\n",
    "                                                     inplace    = True)\n",
    "\n",
    "\n",
    "# LATE_DELIVERIES\n",
    "chef['out_LATE_DELIVERIES']  = 0\n",
    "condition_hi = chef.loc[0:,'out_LATE_DELIVERIES'][chef['LATE_DELIVERIES'] \n",
    "                                                         > late_deliveries_hi]\n",
    "\n",
    "chef['out_LATE_DELIVERIES'].replace(to_replace = condition_hi,\n",
    "                                           value      = 1,\n",
    "                                           inplace    = True)\n",
    "\n",
    "\n",
    "# EARLY_DELIVERIES\n",
    "chef['out_EARLY_DELIVERIES']  = 0\n",
    "condition_lo = chef.loc[0:,'out_EARLY_DELIVERIES'][chef['EARLY_DELIVERIES'] \n",
    "                                                         < early_delivery_lo]\n",
    "\n",
    "chef['out_EARLY_DELIVERIES'].replace(to_replace = condition_lo,\n",
    "                                           value      = 1,\n",
    "                                           inplace    = True)\n",
    "\n",
    "# TOTAL_PHOTOS_VIEWED\n",
    "chef['out_TOTAL_PHOTOS_VIEWED']  = 0\n",
    "condition_lo = chef.loc[0:,'out_TOTAL_PHOTOS_VIEWED'][chef['TOTAL_PHOTOS_VIEWED'] \n",
    "                                                         < total_photos_viewed_lo]\n",
    "\n",
    "chef['out_TOTAL_PHOTOS_VIEWED'].replace(to_replace = condition_lo,\n",
    "                                           value      = 1,\n",
    "                                           inplace    = True)\n",
    "\n",
    "\n",
    "# FOLLOW_REC_PCT\n",
    "chef['out_FOLLOWED_RECOMMENDATIONS_PCT']  = 0\n",
    "condition_hi = chef.loc[0:,'out_FOLLOWED_RECOMMENDATIONS_PCT'][chef['FOLLOWED_RECOMMENDATIONS_PCT'] \n",
    "                                                                     > follow_recommendations_pct_hi  ]\n",
    "condition_lo = chef.loc[0:,'out_FOLLOWED_RECOMMENDATIONS_PCT'][chef['FOLLOWED_RECOMMENDATIONS_PCT'] \n",
    "                                                < follow_recommendations_pct_lo ]\n",
    "\n",
    "\n",
    "chef['out_FOLLOWED_RECOMMENDATIONS_PCT'].replace(to_replace = condition_hi,\n",
    "                                                       value      = 1,\n",
    "                                                       inplace    = True)\n",
    "\n",
    "chef['out_FOLLOWED_RECOMMENDATIONS_PCT'].replace(to_replace = condition_lo,\n",
    "                                  value      = 1,\n",
    "                                  inplace    = True)\n",
    "\n",
    "\n",
    "# PC_LOGINS\n",
    "chef['out_PC_LOGINS'] = 0\n",
    "condition_hi = chef.loc[0:,'out_PC_LOGINS'][chef['PC_LOGINS'] \n",
    "                                            > pc_log_hi ]\n",
    "condition_lo = chef.loc[0:,'out_PC_LOGINS'][chef['PC_LOGINS'] \n",
    "                                                < pc_log_lo ]\n",
    "\n",
    "chef['out_PC_LOGINS'].replace(to_replace = condition_hi,\n",
    "                              value      = 1,\n",
    "                              inplace    = True)\n",
    "chef['out_PC_LOGINS'].replace(to_replace = condition_lo,\n",
    "                             value      = 1,\n",
    "                             inplace    = True)\n",
    "\n",
    "\n",
    "# MOBILE_LOGINS\n",
    "chef['out_MOBILE_LOGINS'] = 0\n",
    "condition_hi = chef.loc[0:,'out_MOBILE_LOGINS'][chef['MOBILE_LOGINS'] \n",
    "                                                > mobile_log_hi ]\n",
    "condition_lo = chef.loc[0:,'out_MOBILE_LOGINS'][chef['MOBILE_LOGINS'] \n",
    "                                                < mobile_log_lo ]\n",
    "\n",
    "chef['out_MOBILE_LOGINS'].replace(to_replace = condition_hi,\n",
    "                                  value      = 1,\n",
    "                                  inplace    = True)\n",
    "chef['out_MOBILE_LOGINS'].replace(to_replace = condition_lo,\n",
    "                                  value      = 1,\n",
    "                                  inplace    = True)\n",
    "\n",
    "# WEEKLY_PLAN\n",
    "chef['out_WEEKLY_PLAN'] = 0\n",
    "condition_hi = chef.loc[0:,'out_WEEKLY_PLAN'][chef['WEEKLY_PLAN'] \n",
    "                                              > weekly_plan_hi]\n",
    "\n",
    "chef['out_WEEKLY_PLAN'].replace(to_replace = condition_hi,\n",
    "                                value      = 1,\n",
    "                                inplace    = True)\n",
    "\n",
    "\n",
    "# AVG_PREP_VID_TIME\n",
    "chef['out_AVG_PREP_VID_TIME'] = 0\n",
    "condition_hi = chef.loc[0:,'out_AVG_PREP_VID_TIME'][chef['AVG_PREP_VID_TIME'] \n",
    "                                                    > avg_prep_video_hi]\n",
    "\n",
    "\n",
    "chef['out_AVG_PREP_VID_TIME'].replace(to_replace = condition_hi,\n",
    "                                      value      = 1,\n",
    "                                      inplace    = True)\n",
    "\n",
    "\n",
    "\n",
    "# TOTAL_PHOTOS_VIEWED\n",
    "chef['out_TOTAL_PHOTOS_VIEWED']  = 0\n",
    "condition_lo = chef.loc[0:,'out_TOTAL_PHOTOS_VIEWED'][chef['TOTAL_PHOTOS_VIEWED'] \n",
    "                                                         < total_photos_viewed_lo]\n",
    "\n",
    "chef['out_TOTAL_PHOTOS_VIEWED'].replace(to_replace = condition_lo,\n",
    "                                           value      = 1,\n",
    "                                           inplace    = True)\n",
    "\n",
    "\n",
    "# PERCENT_UNIQUE_MEALS\n",
    "chef['out_PERCENT_UNIQUE_MEALS'] = 0\n",
    "condition_hi = chef.loc[0:,'out_PERCENT_UNIQUE_MEALS'][chef['PERCENT_UNIQUE_MEALS'] \n",
    "                                                    > percent_unique_meals_hi]\n",
    "\n",
    "\n",
    "chef['out_PERCENT_UNIQUE_MEALS'].replace(to_replace = condition_hi,\n",
    "                                      value      = 1,\n",
    "                                      inplace    = True)\n",
    "\n",
    "# AVG_PRICE_PER_MEAL\n",
    "chef['out_AVG_PRICE_PER_MEAL']  = 0\n",
    "condition_hi = chef.loc[0:,'out_AVG_PRICE_PER_MEAL'][chef['AVG_PRICE_PER_MEAL'] \n",
    "                                                          > avg_meal_price_hi]\n",
    "\n",
    "chef['out_AVG_PRICE_PER_MEAL'].replace(to_replace = condition_hi,\n",
    "                                              value      = 1,\n",
    "                                              inplace    = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Correlation and variable selection </h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Heatmap </h2\n",
    "\n",
    "\n",
    "\n",
    "The heatmap was used a inital measure to get a general feeling of distributions of correlations and to see if anything is immediately noticable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating correlation matrix\n",
    "chef_corr = chef.corr().round(2)\n",
    "\n",
    "# Heatmap gave a good overview over correlation within the dataset\n",
    "#fig, ax = plt.subplots(figsize  = (20,20))\n",
    "\n",
    "#sns.heatmap(chef_corr, cmap = 'coolwarm',\n",
    "            #square = True, annot = True,\n",
    "            #linecolor = 'black', linewidths = 0.5)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Correlation list </h2\n",
    "    \n",
    "First we took a look at all variables correlated with cross sell success. It was found that how often people follow recommendations is the strongest correlated variable(s) with cross sell success. Leading me to my first insight\n",
    "\n",
    "Because of this i also decided to dive deeper and print out the correlations with \"followed_recommendation_pct\" to see what factors correlate with how much a person follows recommendations\n",
    "\n",
    "Finally because the type of email domain seems to be the second biggest correlation with cross sell success. I decided to investigate if there is anything that can potentially correlate with whether someone uses a valid email. (No strong AND logical determination found)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS_SELL_SUCCESS                  1.00\n",
      "NORMALLY_FOLLOWS_RECOMMENDATIONS    0.51\n",
      "out_FOLLOWED_RECOMMENDATIONS_PCT    0.50\n",
      "FOLLOWED_RECOMMENDATIONS_PCT        0.46\n",
      "VALID_EMAIL                         0.28\n",
      "professional                        0.19\n",
      "NUMBER_NAMES                        0.16\n",
      "CANCELLATIONS_BEFORE_NOON           0.16\n",
      "MOBILE_NUMBER                       0.10\n",
      "TASTES_AND_PREFERENCES              0.08\n",
      "REFRIGERATED_LOCKER                 0.07\n",
      "ATTENDED_MASTER_CLASS               0.05\n",
      "CONTACTS_W_CUSTOMER_SERVICE         0.04\n",
      "PC_LOGINS                           0.04\n",
      "PACKAGE_LOCKER                      0.04\n",
      "personal                            0.04\n",
      "MASTER_CLASSES_ATTENDED             0.04\n",
      "out_CANCELLATIONS_BEFORE_NOON       0.03\n",
      "AVG_PREP_VID_TIME                   0.03\n",
      "MEDIAN_MEAL_RATING                  0.03\n",
      "EARLY_DELIVERIES                    0.02\n",
      "NOBLE                               0.02\n",
      "LARGEST_ORDER_SIZE                  0.02\n",
      "out_AVG_PREP_VID_TIME               0.02\n",
      "LATE_DELIVERIES                     0.01\n",
      "AVG_TIME_PER_SITE_VISIT             0.01\n",
      "TOTAL_MEALS_ORDERED                 0.01\n",
      "out_AVG_PRICE_PER_MEAL              0.01\n",
      "TOTAL_PHOTOS_VIEWED                 0.01\n",
      "ORDERED_BEVERAGES                   0.01\n",
      "REVENUE                             0.00\n",
      "PRODUCT_CATEGORIES_VIEWED           0.00\n",
      "out_WEEKLY_PLAN                     0.00\n",
      "UNIQUE_MEALS_PURCH                  0.00\n",
      "out_REVENUE                         0.00\n",
      "WEEKLY_PLAN                        -0.01\n",
      "AVG_PRICE_PER_MEAL                 -0.02\n",
      "out_PERCENT_UNIQUE_MEALS           -0.02\n",
      "out_AVG_TIME_PER_SITE_VISIT        -0.02\n",
      "PERCENT_UNIQUE_MEALS               -0.03\n",
      "out_LATE_DELIVERIES                -0.03\n",
      "out_UNIQUE_MEALS_PURCH             -0.03\n",
      "out_CONTACTS_W_CUSTOMER_SERVICE    -0.03\n",
      "AVG_CLICKS_PER_VISIT               -0.04\n",
      "WILLIGNESS_NEW_PRODUCTS            -0.04\n",
      "out_TOTAL_MEALS_ORDERED            -0.05\n",
      "MOBILE_LOGINS                      -0.05\n",
      "CANCELLATIONS_AFTER_NOON           -0.05\n",
      "junk                               -0.28\n",
      "out_EARLY_DELIVERIES                 NaN\n",
      "out_TOTAL_PHOTOS_VIEWED              NaN\n",
      "out_PC_LOGINS                        NaN\n",
      "out_MOBILE_LOGINS                    NaN\n",
      "Name: CROSS_SELL_SUCCESS, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Looking at correlation with CROSS_SELL_SUCCESS\n",
    "print(chef_corr['CROSS_SELL_SUCCESS'].sort_values(ascending=False))\n",
    "\n",
    "# Finding correlation for FOLLOWED_RECOMMENDATIONS_PCT \n",
    "#print(chef_corr['FOLLOWED_RECOMMENDATIONS_PCT'].sort_values(ascending=False))\n",
    "\n",
    "#print(chef_corr['VALID_EMAIL'].sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop to print the numeric variable in the right format for Statmodel\n",
    "#for col in numerical_chef:\n",
    "    #print(f\"chef['{col}'] +\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Logistic regression </h2\n",
    "    \n",
    "It is here where we added all variables to the model and removed insignificant variables (by p-value) one by one.\n",
    "\n",
    "For several of the new features created the p value deemed the variable to be insignificant. This is because the features used to develop the new features already incapsulated some of this information. Several test were carried out to determine whether the new features were better predictors or the variables the new features were derived from were better and the following combination was selected\n",
    "\n",
    "Even though the final variables selected do not give the highest \"psuedo r-squared\" i believe it was better to carry on with only significant variables\n",
    "\n",
    "<strong> P-value threshold at 0.05 </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.420638\n",
      "         Iterations 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>chef['CROSS_SELL_SUCCESS']</td> <th>  No. Observations:  </th>   <td>  1946</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                      <td>Logit</td>           <th>  Df Residuals:      </th>   <td>  1933</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                      <td>MLE</td>            <th>  Df Model:          </th>   <td>    12</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>                 <td>Sun, 15 Mar 2020</td>      <th>  Pseudo R-squ.:     </th>   <td>0.3299</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                     <td>12:00:37</td>          <th>  Log-Likelihood:    </th>  <td> -818.56</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>                  <td>True</td>            <th>  LL-Null:           </th>  <td> -1221.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>          <td>nonrobust</td>         <th>  LLR p-value:       </th> <td>8.231e-165</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                    <td></td>                      <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                            <td>   -4.3886</td> <td>    0.682</td> <td>   -6.432</td> <td> 0.000</td> <td>   -5.726</td> <td>   -3.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>chef['FOLLOWED_RECOMMENDATIONS_PCT']</th> <td>    0.0575</td> <td>    0.004</td> <td>   16.269</td> <td> 0.000</td> <td>    0.051</td> <td>    0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>chef['MOBILE_NUMBER']</th>                <td>    0.7838</td> <td>    0.181</td> <td>    4.331</td> <td> 0.000</td> <td>    0.429</td> <td>    1.138</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>chef['CANCELLATIONS_BEFORE_NOON']</th>    <td>    0.2609</td> <td>    0.045</td> <td>    5.847</td> <td> 0.000</td> <td>    0.173</td> <td>    0.348</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>chef['TASTES_AND_PREFERENCES']</th>       <td>    0.4680</td> <td>    0.134</td> <td>    3.494</td> <td> 0.000</td> <td>    0.205</td> <td>    0.730</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>chef['REFRIGERATED_LOCKER']</th>          <td>    0.5104</td> <td>    0.206</td> <td>    2.482</td> <td> 0.013</td> <td>    0.107</td> <td>    0.913</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>chef['PC_LOGINS']</th>                    <td>    0.2124</td> <td>    0.106</td> <td>    2.006</td> <td> 0.045</td> <td>    0.005</td> <td>    0.420</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>chef['MOBILE_LOGINS']</th>                <td>   -0.2433</td> <td>    0.116</td> <td>   -2.098</td> <td> 0.036</td> <td>   -0.471</td> <td>   -0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>chef['junk']</th>                         <td>   -1.2905</td> <td>    0.157</td> <td>   -8.218</td> <td> 0.000</td> <td>   -1.598</td> <td>   -0.983</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>chef['professional']</th>                 <td>    0.6692</td> <td>    0.144</td> <td>    4.648</td> <td> 0.000</td> <td>    0.387</td> <td>    0.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>chef['NUMBER_NAMES']</th>                 <td>    0.8708</td> <td>    0.110</td> <td>    7.927</td> <td> 0.000</td> <td>    0.655</td> <td>    1.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>chef['NOBLE']</th>                        <td>   -1.8227</td> <td>    0.386</td> <td>   -4.718</td> <td> 0.000</td> <td>   -2.580</td> <td>   -1.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>chef['WILLIGNESS_NEW_PRODUCTS']</th>      <td>   -0.3583</td> <td>    0.177</td> <td>   -2.019</td> <td> 0.043</td> <td>   -0.706</td> <td>   -0.011</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                               Logit Regression Results                               \n",
       "======================================================================================\n",
       "Dep. Variable:     chef['CROSS_SELL_SUCCESS']   No. Observations:                 1946\n",
       "Model:                                  Logit   Df Residuals:                     1933\n",
       "Method:                                   MLE   Df Model:                           12\n",
       "Date:                        Sun, 15 Mar 2020   Pseudo R-squ.:                  0.3299\n",
       "Time:                                12:00:37   Log-Likelihood:                -818.56\n",
       "converged:                               True   LL-Null:                       -1221.6\n",
       "Covariance Type:                    nonrobust   LLR p-value:                8.231e-165\n",
       "========================================================================================================\n",
       "                                           coef    std err          z      P>|z|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------------------------------\n",
       "Intercept                               -4.3886      0.682     -6.432      0.000      -5.726      -3.051\n",
       "chef['FOLLOWED_RECOMMENDATIONS_PCT']     0.0575      0.004     16.269      0.000       0.051       0.064\n",
       "chef['MOBILE_NUMBER']                    0.7838      0.181      4.331      0.000       0.429       1.138\n",
       "chef['CANCELLATIONS_BEFORE_NOON']        0.2609      0.045      5.847      0.000       0.173       0.348\n",
       "chef['TASTES_AND_PREFERENCES']           0.4680      0.134      3.494      0.000       0.205       0.730\n",
       "chef['REFRIGERATED_LOCKER']              0.5104      0.206      2.482      0.013       0.107       0.913\n",
       "chef['PC_LOGINS']                        0.2124      0.106      2.006      0.045       0.005       0.420\n",
       "chef['MOBILE_LOGINS']                   -0.2433      0.116     -2.098      0.036      -0.471      -0.016\n",
       "chef['junk']                            -1.2905      0.157     -8.218      0.000      -1.598      -0.983\n",
       "chef['professional']                     0.6692      0.144      4.648      0.000       0.387       0.951\n",
       "chef['NUMBER_NAMES']                     0.8708      0.110      7.927      0.000       0.655       1.086\n",
       "chef['NOBLE']                           -1.8227      0.386     -4.718      0.000      -2.580      -1.065\n",
       "chef['WILLIGNESS_NEW_PRODUCTS']         -0.3583      0.177     -2.019      0.043      -0.706      -0.011\n",
       "========================================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Statsmodel to build a linear regression with p-values\n",
    "log_chef_test = smf.logit(formula = \"\"\"chef['CROSS_SELL_SUCCESS']~\n",
    "                                        \n",
    "chef['FOLLOWED_RECOMMENDATIONS_PCT'] +\n",
    "chef['MOBILE_NUMBER'] +\n",
    "chef['CANCELLATIONS_BEFORE_NOON'] +\n",
    "chef['TASTES_AND_PREFERENCES'] +\n",
    "chef['REFRIGERATED_LOCKER'] +\n",
    "chef['PC_LOGINS'] +\n",
    "chef['MOBILE_LOGINS'] +\n",
    "chef['junk'] +\n",
    "chef['professional'] +\n",
    "chef['NUMBER_NAMES'] +\n",
    "chef['NOBLE'] +\n",
    "chef['WILLIGNESS_NEW_PRODUCTS'] \n",
    "        \"\"\", data = chef)\n",
    "\n",
    "\n",
    "result_fitted_chef_test = log_chef_test.fit()\n",
    "\n",
    "result_fitted_chef_test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Candidate dictionary </h2\n",
    "    \n",
    " Two candidate dictionaries developed. One containing only the significant variables determine in step 4.4. above. The other dictionary contains all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable dictionary for signingicant variables and the full dataset\n",
    "\n",
    "variable_dict = {\n",
    "    'logit_sig' : ['MOBILE_NUMBER',\n",
    "                   'CANCELLATIONS_BEFORE_NOON' ,\n",
    "                   'TASTES_AND_PREFERENCES' ,\n",
    "                   'PC_LOGINS',\n",
    "                   'MOBILE_LOGINS' ,\n",
    "                   'junk',\n",
    "                   'professional',\n",
    "                   'NUMBER_NAMES',\n",
    "                   'NOBLE',\n",
    "                   'REFRIGERATED_LOCKER' ,\n",
    "                   'WILLIGNESS_NEW_PRODUCTS',\n",
    "                   'FOLLOWED_RECOMMENDATIONS_PCT'],\n",
    "    \n",
    "    \n",
    "   'logit_full' : ['REVENUE','TOTAL_MEALS_ORDERED' ,\n",
    "                   'UNIQUE_MEALS_PURCH' ,'CONTACTS_W_CUSTOMER_SERVICE',\n",
    "                   'PRODUCT_CATEGORIES_VIEWED' ,'AVG_TIME_PER_SITE_VISIT',\n",
    "                   'MOBILE_NUMBER' ,'CANCELLATIONS_BEFORE_NOON' ,'CANCELLATIONS_AFTER_NOON',\n",
    "                   'TASTES_AND_PREFERENCES' ,'PC_LOGINS' ,\n",
    "                   'MOBILE_LOGINS','WEEKLY_PLAN' ,'EARLY_DELIVERIES','LATE_DELIVERIES',\n",
    "                   'PACKAGE_LOCKER','REFRIGERATED_LOCKER',\n",
    "                   'FOLLOWED_RECOMMENDATIONS_PCT' ,'AVG_PREP_VID_TIME' ,\n",
    "                   'LARGEST_ORDER_SIZE' ,'MEDIAN_MEAL_RATING' ,'AVG_CLICKS_PER_VISIT',\n",
    "                   'TOTAL_PHOTOS_VIEWED','AVG_PRICE_PER_MEAL','junk' ,'personal',\n",
    "                   'professional' ,'VALID_EMAIL',\n",
    "                   'NUMBER_NAMES' ,'ATTENDED_MASTER_CLASS' ,'NOBLE',\n",
    "                   'PERCENT_UNIQUE_MEALS' , 'WILLIGNESS_NEW_PRODUCTS' ,\n",
    "                   'NORMALLY_FOLLOWS_RECOMMENDATIONS','ORDERED_BEVERAGES',\n",
    "                   'out_REVENUE','out_TOTAL_MEALS_ORDERED','out_UNIQUE_MEALS_PURCH',\n",
    "                   'out_CONTACTS_W_CUSTOMER_SERVICE' ,'out_AVG_TIME_PER_SITE_VISIT','out_CANCELLATIONS_BEFORE_NOON',\n",
    "                   'out_LATE_DELIVERIES' ,'out_EARLY_DELIVERIES','out_TOTAL_PHOTOS_VIEWED' ,\n",
    "                   'out_FOLLOWED_RECOMMENDATIONS_PCT','out_PC_LOGINS' ,'out_MOBILE_LOGINS' ,'out_WEEKLY_PLAN',\n",
    "                   'out_AVG_PREP_VID_TIME','out_PERCENT_UNIQUE_MEALS', 'out_AVG_PRICE_PER_MEAL']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into a train and test set for the statsmodel\n",
    "\n",
    "#chef_data   = chef.drop(columns = 'CROSS_SELL_SUCCESS')\n",
    "\n",
    "#chef_target = chef.loc[:,'CROSS_SELL_SUCCESS']\n",
    "\n",
    "# preparing training and testing sets (Training = 75% , Testing = 25%)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(chef_data,\n",
    "                                                   #chef_target,\n",
    "                                                   #test_size = 0.25,\n",
    "                                                   #random_state = 222,\n",
    "                                                   #stratify =chef_target)\n",
    "\n",
    "# merging training data for statsmodels since it doesn't work the same way as sci-kit learn\n",
    "#chef_train = pd.concat([X_train, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Model development </h1>\n",
    "\n",
    "Created two training and testing sets based on the significant variables listed in the candidate dictionary\n",
    "\n",
    "Set 1 - stratifys on 'FOLLOWED_RECOMMENDATIONS_PCT' because it is the most important variable and will help the model if the distribution of variables within is equal\n",
    "\n",
    "Set 2 - The second set scales the explantory data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Set 1 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating data and target variables\n",
    "chef_data   =  chef.loc[ : , variable_dict['logit_sig']]\n",
    "chef_target =  chef.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "\n",
    "# preparing training and testing sets (Training = 75% , Testing = 25%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            chef_data,\n",
    "            chef_target,\n",
    "            random_state = 222,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = chef['FOLLOWED_RECOMMENDATIONS_PCT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Set 2 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating data and target variables\n",
    "chef_data   =  chef.loc[ : , variable_dict['logit_sig']]\n",
    "chef_target =  chef.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "\n",
    "# INSTANTIATING StandardScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# FITTING the independent variable data\n",
    "scaler.fit(chef_data)\n",
    "\n",
    "\n",
    "# TRANSFORMING the independent variable data\n",
    "X_scaled     = scaler.transform(chef_data)\n",
    "\n",
    "\n",
    "# converting to a DataFrame\n",
    "X_scaled_df  = pd.DataFrame(X_scaled) \n",
    "\n",
    "\n",
    "# Train test split with all the scaled data\n",
    "X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(\n",
    "                    X_scaled_df,\n",
    "                    chef_target,\n",
    "                    random_state = 222,\n",
    "                    test_size = 0.25,\n",
    "                    stratify = chef_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Model performance </h3>\n",
    "\n",
    "Creating a list that will be called on later to quickly see which model performed best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a list to keep track of model values\n",
    "model_performance = [['Model', 'Training Accuracy',\n",
    "                      'Testing Accuracy', 'AUC Value']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Testing on significant variables </h2>\n",
    "\n",
    "Each model will be tested on both sets of training and testing data outlined inn section 5.0\n",
    "\n",
    "The models used in this Analysis include\n",
    "\n",
    "1) Logistic\n",
    "\n",
    "2) CART\n",
    "\n",
    "3) Gradient Boost \n",
    "\n",
    "4) KNN\n",
    "\n",
    "<h3> Logistic model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7807\n",
      "Testing  ACCURACY: 0.7803\n",
      "AUC Score        : 0.7504980484631647\n"
     ]
    }
   ],
   "source": [
    "#INSTANTIATING a logistic regression model\n",
    "logreg = LogisticRegression(solver = 'liblinear',\n",
    "                                 C = 1,\n",
    "                      random_state = 222)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "logreg_fit = logreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "logreg_pred = logreg_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', logreg_fit.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', logreg_fit.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = logreg_pred))\n",
    "\n",
    "# Adding model results to table\n",
    "model_performance.append(['Logistic Regression w significant var',\n",
    "                          logreg_fit.score(X_train, y_train).round(4),\n",
    "                          logreg_fit.score(X_test, y_test).round(4),\n",
    "                          roc_auc_score(y_true  = y_test,\n",
    "                                        y_score = logreg_pred).round(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7985\n",
      "Testing  ACCURACY: 0.7474\n",
      "AUC Score        : 0.7057479278023084\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a logistic regression model\n",
    "logreg = LogisticRegression(solver = 'liblinear',\n",
    "                            C = 1,\n",
    "                            random_state = 222)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "logreg_fit = logreg.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "logreg_pred = logreg_fit.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', logreg_fit.score(X_train_scaled, y_train_scaled).round(4))\n",
    "print('Testing  ACCURACY:', logreg_fit.score(X_test_scaled, y_test_scaled).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test_scaled,\n",
    "                                          y_score = logreg_pred))\n",
    "\n",
    "# Adding model results to table\n",
    "model_performance.append(['Logistic Regression scaled significant var',\n",
    "                          logreg_fit.score(X_train_scaled, y_train_scaled).round(4),\n",
    "                          logreg_fit.score(X_test_scaled, y_test_scaled).round(4),\n",
    "                          roc_auc_score(y_true  = y_test_scaled,\n",
    "                                        y_score = logreg_pred).round(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> CART model</h3>\n",
    "\n",
    "<h4> Two user defined functions developed  </h4>\n",
    "\n",
    "display_tree - To print out tree model\n",
    "\n",
    "feature_importances - To print out bar graph of the most important features in the CART model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# display_tree\n",
    "########################################\n",
    "def display_tree(tree, feature_df, height = 500, width = 800):\n",
    "    \"\"\"\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    tree       : fitted tree model object\n",
    "        fitted CART model to visualized\n",
    "    feature_df : DataFrame\n",
    "        DataFrame of explanatory features (used to generate labels)\n",
    "    height     : int, default 500\n",
    "        height in pixels to which to constrain image in html\n",
    "    width      : int, default 800\n",
    "        width in pixels to which to constrain image in html\n",
    "    \"\"\"\n",
    "\n",
    "    # visualizing the tree\n",
    "    dot_data = StringIO()\n",
    "\n",
    "    \n",
    "    # exporting tree to graphviz\n",
    "    export_graphviz(decision_tree      = tree,\n",
    "                    out_file           = dot_data,\n",
    "                    filled             = True,\n",
    "                    rounded            = True,\n",
    "                    special_characters = True,\n",
    "                    feature_names      = feature_df.columns)\n",
    "\n",
    "\n",
    "    # declaring a graph object\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "\n",
    "\n",
    "    # creating image\n",
    "    img = Image(graph.create_png(),\n",
    "                height = height,\n",
    "                width  = width)\n",
    "    \n",
    "    return img\n",
    "\n",
    "########################################\n",
    "# plot_feature_importances\n",
    "########################################\n",
    "def plot_feature_importances(model, train, export = False):\n",
    "    \"\"\"\n",
    "    Plots the importance of features from a CART model.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    model  : CART model\n",
    "    train  : explanatory variable training data\n",
    "    export : whether or not to export as a .png image, default False\n",
    "    \"\"\"\n",
    "    \n",
    "    # declaring the number\n",
    "    n_features = X_train.shape[1]\n",
    "    \n",
    "    # setting plot window\n",
    "    fig, ax = plt.subplots(figsize=(12,9))\n",
    "    \n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(pd.np.arange(n_features), train.columns)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    \n",
    "    if export == True:\n",
    "        plt.savefig('Tree_Leaf_50_Feature_Importance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Initial CART model on Set 1 </h4>\n",
    "\n",
    "It was found that the default hyperparameters were creating too large of a tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.9596\n",
      "Testing  ACCURACY: 0.7536\n",
      "AUC Score        : 0.7377\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "tree_pruned      = DecisionTreeClassifier()             \n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "tree_pruned_fit  = tree_pruned.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING test data set\n",
    "tree_pred = tree_pruned_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Training ACCURACY:', tree_pruned_fit.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', tree_pruned_fit.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = tree_pred).round(4))\n",
    "\n",
    "# Adding model results to table\n",
    "model_performance.append(['Decision Tree w significant var',\n",
    "                          tree_pruned_fit.score(X_train, y_train).round(4),\n",
    "                          tree_pruned_fit.score(X_test, y_test).round(4),\n",
    "                          roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = tree_pred).round(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Hyperparameter tuning for CART model on Set 1 </h4>\n",
    "\n",
    "\n",
    "The cell was ran and the hyperparameters produced are given below\n",
    "\n",
    "Tuned Parameters  :<strong> Tuned Parameters  : {'max_depth': 4, 'min_samples_leaf': 0.11, 'min_samples_split': 0.01}\n",
    "Tuned CV AUC      : 0.7638 </strong> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring a hyperparameter space\n",
    "#max_depth          = pd.np.arange(1, 10, 1)\n",
    "#min_samples_split = pd.np.arange(0.01, 1, 0.05)\n",
    "#min_samples_leaf = pd.np.arange(0.01, 0.5, 0.05)\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "#params = {'max_depth': max_depth,\n",
    "          #'min_samples_split': min_samples_split,\n",
    "          #'min_samples_leaf': min_samples_leaf }\n",
    "\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "#tree = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "#tree_cv = GridSearchCV(tree, \n",
    "                       #param_grid= params, \n",
    "                       #n_jobs= -1, \n",
    "                       #cv = 3,\n",
    "                       #scoring = make_scorer(roc_auc_score,needs_threshold = False))\n",
    "\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "#tree_cv = tree_cv.fit(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "#print(\"Tuned Parameters  :\", tree_cv.best_params_)\n",
    "#print(\"Tuned CV AUC      :\", tree_cv.best_score_.round(4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Tuned Tree on set 1 </h4>\n",
    "\n",
    "The best hyperparameters from the cell above were manually entered and used to produce a tuned tree \n",
    "\n",
    "Because this was the <strong> Best CART model </strong> there is an option to print a visual of the tree as well as a bar graph of feature importances (followed_recommendations_pct was by far the most important feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.8081\n",
      "Testing  ACCURACY: 0.8111\n",
      "AUC Score        : 0.7723\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a logistic regression model with tuned values\n",
    "tree_tuned = DecisionTreeClassifier( \n",
    "                                    max_depth = 4,\n",
    "                                    min_samples_leaf = 0.11,\n",
    "                                    min_samples_split = 0.01,\n",
    "                                    random_state = 222)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "tree_tuned_fit  = tree_tuned.fit(X_train, y_train)\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "tree_tuned_pred = tree_tuned_fit.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', tree_tuned.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', tree_tuned.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                  y_score = tree_tuned_pred).round(4))\n",
    "\n",
    "# Adding model results to table\n",
    "model_performance.append(['Tuned Tree w significant var',\n",
    "                          tree_tuned.score(X_train, y_train).round(4),\n",
    "                          tree_tuned.score(X_test, y_test).round(4),\n",
    "                          roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = tree_pred).round(4)])\n",
    "\n",
    "\n",
    "\n",
    "# calling display_tree\n",
    "#display_tree(tree       = tree_tuned,\n",
    "             #feature_df = X_train)\n",
    "\n",
    "    \n",
    "    \n",
    "# calling plot_feature_importances\n",
    "#plot_feature_importances(tree_tuned_fit,\n",
    "                         #train = X_train,\n",
    "                         #export = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Initial CART model on Set 2 </h4>\n",
    "\n",
    "*Remember set 2 is the scaled version of the training and testing set\n",
    "\n",
    "It was found that the default hyperparameters were creating too large of a tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.9575\n",
      "Testing  ACCURACY: 0.7474\n",
      "AUC Score        : 0.7142\n"
     ]
    }
   ],
   "source": [
    "full_tree = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "full_tree_fit = full_tree.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "full_tree_pred = full_tree_fit.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Training ACCURACY:', full_tree_fit.score(X_train_scaled, y_train_scaled).round(4))\n",
    "print('Testing  ACCURACY:', full_tree_fit.score(X_test_scaled, y_test_scaled).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test_scaled,\n",
    "                                          y_score = full_tree_pred).round(4))\n",
    "\n",
    "# Adding model results to table\n",
    "model_performance.append(['Decision tree w scaled var',\n",
    "                          full_tree_fit.score(X_train_scaled, y_train_scaled).round(4),\n",
    "                          full_tree_fit.score(X_test_scaled, y_test_scaled).round(4),\n",
    "                          roc_auc_score(y_true  = y_test_scaled,\n",
    "                                          y_score = full_tree_pred).round(4)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Hyperparameter tuning for CART model on Set 2 </h4>\n",
    "\n",
    "\n",
    "The cell was ran and the hyperparameters produced are given below\n",
    "\n",
    "Tuned Parameters  :<strong> Tuned Parameters  : {'max_depth': 6, 'min_samples_leaf': 0.01, 'min_samples_split': 0.11}\n",
    "Tuned CV AUC      : 0.8021 </strong> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring a hyperparameter space\n",
    "#max_depth          = pd.np.arange(1, 10, 1)\n",
    "#min_samples_split = pd.np.arange(0.01, 1, 0.05)\n",
    "#min_samples_leaf = pd.np.arange(0.01, 0.5, 0.05)\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "#params = {'max_depth': max_depth,\n",
    "          #'min_samples_split': min_samples_split,\n",
    "          #'min_samples_leaf': min_samples_leaf }\n",
    "\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "#tree = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "#tree_cv = GridSearchCV(tree, \n",
    "                       #param_grid= params, \n",
    "                       #n_jobs= -1, \n",
    "                       #cv = 3,\n",
    "                       #scoring = make_scorer(roc_auc_score,needs_threshold = False))\n",
    "\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "#tree_cv = tree_cv.fit(X_test_scaled, y_test_scaled)\n",
    "\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "#print(\"Tuned Parameters  :\", tree_cv.best_params_)\n",
    "#print(\"Tuned CV AUC      :\", tree_cv.best_score_.round(4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Tuned Tree on set 2 </h4>\n",
    "\n",
    "The best hyperparameters from the cell above are used to produce a tuned tree and a option to visual of the tree is printed at the end\n",
    "\n",
    "This produced the . It seems tuning the tree on the scaled data does not improve the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.817\n",
      "Testing  ACCURACY: 0.7844\n",
      "AUC Score        : 0.738\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING the model object withhyperparameters\n",
    "tree_tuned =  DecisionTreeClassifier( \n",
    "                                    max_depth = 6,\n",
    "                                    min_samples_leaf = 0.11,\n",
    "                                    min_samples_split = 0.01,\n",
    "                                    random_state = 222)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "tree_tuned_fit  = tree_tuned.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "tree_tuned_pred = tree_tuned_fit.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', tree_tuned.score(X_train_scaled, y_train_scaled).round(4))\n",
    "print('Testing  ACCURACY:', tree_tuned.score(X_test_scaled, y_test_scaled).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test_scaled,\n",
    "                                  y_score = tree_tuned_pred).round(4))\n",
    "\n",
    "# Adding model results to table\n",
    "model_performance.append(['Tuned Tree w scaled var',\n",
    "                          tree_tuned.score(X_train_scaled, y_train_scaled).round(4),\n",
    "                          tree_tuned.score(X_test_scaled, y_test_scaled).round(4),\n",
    "                          roc_auc_score(y_true  = y_test_scaled,\n",
    "                                          y_score = tree_pred).round(4)])\n",
    "\n",
    "# displaying the tree\n",
    "# calling display_tree\n",
    "#display_tree(tree       = tree_tuned,\n",
    "             #feature_df = X_train_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Gradient Boosting </h3>\n",
    "\n",
    "Tne next model developed was a using the gradient boost classifier. Again similarly to all models. The model was ran twice  on Set 1 and Set 2\n",
    "\n",
    "<h4> Intital Gradient Boost on Set 1  </h4>\n",
    "\n",
    "After trial and error experimentationm, the hyperparameters used from this step managed to produce <strong> the highest AUC score in this analysis (into the 80's!) </strong>\n",
    "\n",
    "These hyperparameters managed to produce a higher AUC score than the tuned hyperparameters developed in the coming steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.8328\n",
      "Testing  ACCURACY: 0.8296\n",
      "AUC Score        : 0.8181\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a classification \n",
    "g_boost = GradientBoostingClassifier(loss = 'deviance',\n",
    "                                     criterion = 'mae',\n",
    "                                     learning_rate =  0.1,\n",
    "                                     n_estimators = 95,\n",
    "                                     max_features = 3,\n",
    "                                     random_state  = 222)\n",
    "\n",
    "# FITTING the training data\n",
    "g_boost_fit = g_boost.fit(X_train, y_train)\n",
    "\n",
    "# PREDICTING on test data\n",
    "g_boost_pred = g_boost_fit.predict(X_test)\n",
    "\n",
    "# SCORING the model\n",
    "print('Training ACCURACY:', g_boost_fit.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', g_boost_fit.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = g_boost_pred).round(4))\n",
    "\n",
    "# Adding model results to table\n",
    "model_performance.append(['GradientBoosting w significant var',\n",
    "                          g_boost_fit.score(X_train, y_train).round(4),\n",
    "                          g_boost_fit.score(X_test, y_test).round(4),\n",
    "                          roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = g_boost_pred).round(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Gradient Boost Hyperparameter tuning on Set 1  </h4>\n",
    "\n",
    "The following results were produced\n",
    "\n",
    "Tuned Parameters <strong>  : {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 200}\n",
    "Tuned Training AUC: 0.7941 </strong> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring a hyperparameter space\n",
    "#learn_space     = pd.np.arange(0.1, 1.6, 0.3)\n",
    "#estimator_space = pd.np.arange(50, 250, 50)\n",
    "#depth_space     = pd.np.arange(1, 10)\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "#param_grid = {'learning_rate' : learn_space,\n",
    "              #'max_depth'     : depth_space,\n",
    "              #'n_estimators'  : estimator_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "#full_gbm_grid = GradientBoostingClassifier(random_state = 802)\n",
    "\n",
    "\n",
    "# GridSearchCV full\n",
    "#full_gbm_cv = GridSearchCV(estimator  = full_gbm_grid,\n",
    "                           #param_grid = param_grid,\n",
    "                           #cv         = 3,\n",
    "                           #scoring    = make_scorer(roc_auc_score,\n",
    "                                        #xneeds_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "#full_gbm_cv.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "#print(\"Tuned Parameters  :\", full_gbm_cv.best_params_)\n",
    "#print(\"Tuned Training AUC:\", full_gbm_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Using hyperparameters on Set 1 </h4>\n",
    "\n",
    "The hyperparameters used from the step above were manually fed in and suprisingly lead to a lower AUC score\n",
    "\n",
    "This leads me to think about the limitations of hyperparameter tuning and whether there are other factors/reasonings for this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.8026\n",
      "Testing  ACCURACY: 0.7906\n",
      "AUC Score        : 0.7414\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a classification \n",
    "g_boost = GradientBoostingClassifier(loss = 'deviance',\n",
    "                                     criterion = 'mae',\n",
    "                                     learning_rate =  0.1,\n",
    "                                     n_estimators = 200,\n",
    "                                     max_depth = 1,\n",
    "                                     max_features = 3,\n",
    "                                     random_state  = 222)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "g_boost_fit = g_boost.fit(X_train, y_train)\n",
    "\n",
    "# PREDICTING on test data\n",
    "g_boost_pred = g_boost_fit.predict(X_test)\n",
    "\n",
    "# SCORING the model\n",
    "print('Training ACCURACY:', g_boost_fit.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', g_boost_fit.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = g_boost_pred).round(4))\n",
    "\n",
    "# Adding model results to table\n",
    "model_performance.append(['Tuned GradientBoosting w significant var',\n",
    "                          g_boost_fit.score(X_train, y_train).round(4),\n",
    "                          g_boost_fit.score(X_test, y_test).round(4),\n",
    "                          roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = g_boost_pred).round(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Intital Gradient Boost on Set 2  </h4>\n",
    "\n",
    "This model gave a relatively low AUC score. Hyperparameter tuning was done again but did not seem to raise AUC score significantly and was therefore removed from the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.8424\n",
      "Testing  ACCURACY: 0.8172\n",
      "AUC Score        : 0.7791\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a classification \n",
    "g_boost = GradientBoostingClassifier(loss = 'deviance',\n",
    "                                     criterion = 'mae',\n",
    "                                     learning_rate =  0.1,\n",
    "                                     n_estimators = 95,\n",
    "                                     max_features = 3,\n",
    "                                     random_state  = 222)\n",
    "\n",
    "# FITTING the training data\n",
    "g_boost_fit = g_boost.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# PREDICTING on test data\n",
    "g_boost_pred = g_boost_fit.predict(X_test_scaled)\n",
    "\n",
    "# SCORING the model\n",
    "print('Training ACCURACY:', g_boost_fit.score(X_train_scaled, y_train_scaled).round(4))\n",
    "print('Testing  ACCURACY:', g_boost_fit.score(X_test_scaled, y_test_scaled).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test_scaled,\n",
    "                                          y_score = g_boost_pred).round(4))\n",
    "\n",
    "# Adding model results to table\n",
    "model_performance.append(['GradientBoosting scaled w scaled var',\n",
    "                          g_boost_fit.score(X_train_scaled, y_train_scaled).round(4),\n",
    "                          g_boost_fit.score(X_test_scaled, y_test_scaled).round(4),\n",
    "                          roc_auc_score(y_true  = y_test_scaled,\n",
    "                                          y_score = g_boost_pred).round(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Random Forest </h3>\n",
    "\n",
    "Tne next model developed was a using the Random Forrest classifier. Again similarly to all models. The model was ran twice  on Set 1 and Set 2\n",
    "\n",
    "<h4> Intital Random Forrest on Set 1  </h4>\n",
    "\n",
    "The following model was ran with random hyperparameters selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.8245\n",
      "Testing  ACCURACY: 0.8111\n",
      "AUC Score        : 0.7478\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "rndfor = RandomForestClassifier(criterion = 'gini',\n",
    "                                bootstrap = True, \n",
    "                                max_depth = 4, \n",
    "                                n_estimators = 200,\n",
    "                                min_samples_leaf = 25, \n",
    "                                random_state = 222)\n",
    "\n",
    "# FITTING the training data\n",
    "rndfor_fit = rndfor.fit(X_train, y_train)\n",
    "\n",
    "# PREDICTING on test data\n",
    "rndfor_pred = rndfor_fit.predict(X_test)\n",
    "\n",
    "# SCORING the model\n",
    "print('Training ACCURACY:', rndfor_fit.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', rndfor_fit.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = rndfor_pred).round(4))\n",
    "\n",
    "model_performance.append(['Random Forrest w significant var',\n",
    "                          rndfor_fit.score(X_train, y_train).round(4),\n",
    "                          rndfor_fit.score(X_test, y_test).round(4),\n",
    "                          roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = rndfor_pred).round(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Random Forrest Hyperparameter tuning on Set 1  </h4>\n",
    "\n",
    "The following results were produced\n",
    "\n",
    "Tuned Parameters  : {'bootstrap': True, 'criterion': 'entropy', 'min_samples_leaf': 11, 'n_estimators': 100, 'warm_start': True}\n",
    "Tuned Training AUC: 0.7787"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring a hyperparameter space\n",
    "#estimator_space  = pd.np.arange(100, 1100, 250)\n",
    "#leaf_space       = pd.np.arange(1, 31, 10)\n",
    "#criterion_space  = ['gini', 'entropy']\n",
    "#bootstrap_space  = [True, False]\n",
    "#warm_start_space = [True, False]\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "#param_grid = {'n_estimators'     : estimator_space,\n",
    "              #'min_samples_leaf' : leaf_space,\n",
    "              #'criterion'        : criterion_space,\n",
    "              #'bootstrap'        : bootstrap_space,\n",
    "              #'warm_start'       : warm_start_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "#full_forest_grid = RandomForestClassifier(random_state = 802)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "#full_forest_cv = GridSearchCV(estimator  = full_forest_grid,\n",
    "                              #param_grid = param_grid,\n",
    "                              #cv         = 3,\n",
    "                              #scoring    = make_scorer(roc_auc_score,\n",
    "                                           #needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "#full_forest_cv.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "#print(\"Tuned Parameters  :\", full_forest_cv.best_params_)\n",
    "#print(\"Tuned Training AUC:\", full_forest_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Using hyperparameters on Set 1 </h4>\n",
    "\n",
    "The hyperparameters used from the step above were manually fed in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.8355\n",
      "Testing  ACCURACY: 0.8193\n",
      "AUC Score        : 0.7761\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "rndfor = RandomForestClassifier(criterion = 'entropy',\n",
    "                                bootstrap = True,  \n",
    "                                n_estimators = 100,\n",
    "                                min_samples_leaf = 11,\n",
    "                                warm_start = True,\n",
    "                                random_state = 222)\n",
    "\n",
    "# FITTING the training data\n",
    "rndfor_fit = rndfor.fit(X_train, y_train)\n",
    "\n",
    "# PREDICTING on test data\n",
    "rndfor_pred = rndfor_fit.predict(X_test)\n",
    "\n",
    "# SCORING the model\n",
    "print('Training ACCURACY:', rndfor_fit.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', rndfor_fit.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = rndfor_pred).round(4))\n",
    "\n",
    "model_performance.append(['Tuned Random Forrest w significant var',\n",
    "                          rndfor_fit.score(X_train, y_train).round(4),\n",
    "                          rndfor_fit.score(X_test, y_test).round(4),\n",
    "                          roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = rndfor_pred).round(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Intital Random Forrest on Set 2  </h4>\n",
    "\n",
    "This model gave a relatively low AUC score. Hyperparameter tuning was done again but did not seem to raise AUC score significantly and was therefore removed from the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.8259\n",
      "Testing  ACCURACY: 0.7926\n",
      "AUC Score        : 0.7254\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "rndfor = RandomForestClassifier(criterion = 'gini',\n",
    "                                bootstrap = True, \n",
    "                                max_depth = 4, \n",
    "                                n_estimators = 200,\n",
    "                                min_samples_leaf = 25, \n",
    "                                random_state = 222)\n",
    "\n",
    "# FITTING the training data\n",
    "rndfor_fit = rndfor.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# PREDICTING on test data\n",
    "rndfor_pred = rndfor_fit.predict(X_test_scaled)\n",
    "\n",
    "# SCORING the model\n",
    "print('Training ACCURACY:', rndfor_fit.score(X_train_scaled, y_train_scaled).round(4))\n",
    "print('Testing  ACCURACY:', rndfor_fit.score(X_test_scaled, y_test_scaled).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test_scaled,\n",
    "                                          y_score = rndfor_pred).round(4))\n",
    "\n",
    "model_performance.append(['Random Forrest scaled w significant var',\n",
    "                          rndfor_fit.score(X_train_scaled, y_train_scaled).round(4),\n",
    "                          rndfor_fit.score(X_test_scaled, y_test_scaled).round(4),\n",
    "                          roc_auc_score(y_true  = y_test_scaled,\n",
    "                                          y_score = rndfor_pred).round(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> KNN </h3>\n",
    "\n",
    "NB: No hyperparameter tuning was developed for this model\n",
    "\n",
    "<h4> User defined functions developed  </h4>\n",
    "\n",
    "Optimal Neighbors - Computes training and testing results for KNN as well as the optimal number of neighbors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User defined function to find the optimal number of neighbors\n",
    "def optimal_neighbors(X_data,\n",
    "                      y_data,\n",
    "                      pct_test=0.25,\n",
    "                      seed=222,\n",
    "                      response_type='class',\n",
    "                      max_neighbors=50):\n",
    "    \"\"\"\n",
    "Exhaustively compute training and testing results for KNN across\n",
    "[1, max_neighbors]. Outputs the maximum test score and (by default) a\n",
    "visualization of the results.\n",
    "PARAMETERS\n",
    "----------\n",
    "X_data        : explanatory variable data\n",
    "y_data        : response variable\n",
    "pct_test      : test size for training and validation from (0,1), default 0.25\n",
    "seed          : random seed to be used in algorithm, default 802\n",
    "response_type : type of neighbors algorithm to use, default 'reg'\n",
    "    Use 'reg' for regression (KNeighborsRegressor)\n",
    "    Use 'class' for classification (KNeighborsClassifier)\n",
    "max_neighbors : maximum number of neighbors in exhaustive search, default 20\n",
    "\"\"\"    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data,\n",
    "                                                        y_data,\n",
    "                                                        test_size = pct_test,\n",
    "                                                        random_state = seed)\n",
    "\n",
    "\n",
    "    # creating lists for training set accuracy and test set accuracy\n",
    "    training_accuracy = []\n",
    "    test_accuracy = []\n",
    "    \n",
    "    \n",
    "    # setting neighbor range\n",
    "    neighbors_settings = range(1, max_neighbors + 1)\n",
    "\n",
    "\n",
    "    for n_neighbors in neighbors_settings:\n",
    "        # building the model based on response variable type\n",
    "        if response_type == 'reg':\n",
    "            clf = KNeighborsRegressor(n_neighbors = n_neighbors)\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "        elif response_type == 'class':\n",
    "            clf = KNeighborsClassifier(n_neighbors = n_neighbors)\n",
    "            clf.fit(X_train, y_train)            \n",
    "            \n",
    "        else:\n",
    "            print(\"Error: response_type must be 'reg' or 'class'\")\n",
    "        \n",
    "        \n",
    "        # recording the training set accuracy\n",
    "        training_accuracy.append(clf.score(X_train, y_train))\n",
    "    \n",
    "        # recording the generalization accuracy\n",
    "        test_accuracy.append(clf.score(X_test, y_test))\n",
    "    \n",
    "    # returning optimal number of neighbors\n",
    "    print(f\"The optimal number of neighbors is: {test_accuracy.index(max(test_accuracy))+1}\")\n",
    "    return test_accuracy.index(max(test_accuracy))+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Intital KNN on Set 1  </h4>\n",
    "\n",
    "The following model was ran with the optimal number of neighbors (33) \n",
    "\n",
    "<strong> This produced the second best model </strong> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal number of neighbors is: 33\n",
      "Training ACCURACY: 0.8232\n",
      "Testing  ACCURACY: 0.7988\n",
      "AUC Score        : 0.7942\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a KNN classification model with optimal neighbors\n",
    "knn_opt = KNeighborsClassifier(n_neighbors = optimal_neighbors(X_train, \n",
    "                                                               y_train))\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "knn_fit = knn_opt.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "knn_pred = knn_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', knn_fit.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', knn_fit.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = knn_pred).round(4))\n",
    "\n",
    "model_performance.append(['KNeighbor w significant var',\n",
    "                          knn_fit.score(X_train, y_train).round(4),\n",
    "                          knn_fit.score(X_test, y_test).round(4),\n",
    "                          roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = knn_pred).round(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Intital KNN on Set 2  </h4>\n",
    "\n",
    "Running the same model on the scaled variables brough the AUC score lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal number of neighbors is: 20\n",
      "Training ACCURACY: 0.804\n",
      "Testing  ACCURACY: 0.7659\n",
      "AUC Score        : 0.716\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a KNN classification model with optimal neighbors\n",
    "knn_opt = KNeighborsClassifier(n_neighbors = optimal_neighbors(X_train_scaled, \n",
    "                                                               y_train_scaled))\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "knn_fit = knn_opt.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "knn_pred = knn_fit.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', knn_fit.score(X_train_scaled, y_train_scaled).round(4))\n",
    "print('Testing  ACCURACY:', knn_fit.score(X_test_scaled, y_test_scaled).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test_scaled,\n",
    "                                          y_score = knn_pred).round(4))\n",
    "\n",
    "model_performance.append(['KNeighbor Scaled w scaled var',\n",
    "                          knn_fit.score(X_train_scaled, y_train_scaled).round(4),\n",
    "                          knn_fit.score(X_test_scaled, y_test_scaled).round(4),\n",
    "                          roc_auc_score(y_true  = y_test_scaled,\n",
    "                                          y_score = knn_pred).round(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Testing on <strong> ALL </strong> variables (i.e. logit_full) </h2>\n",
    "\n",
    "The following tests were done out of experimentation to see if the significant variables we determined in Section 4.5 were better than using all variables. Therefore the 3 best performing model types so far were ran\n",
    "\n",
    "Since all previous models ran better with Set 1 i.e. stratified with FOLLOWED_RECOMMENDATIONS_PCT, it was decided to save time and computing power and only run this Set with the logit_full. \n",
    "\n",
    "Moreover these models were not hyperparameter tuned since we expect to have lower AUC scores here than with the significant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding in all variables to see the ensemble models can do a better job with all\n",
    "chef_data   =  chef[variable_dict['logit_full']]\n",
    "chef_target =  chef.loc[:,'CROSS_SELL_SUCCESS']\n",
    "\n",
    "\n",
    "# this is the exact code we were using before\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            chef_data,\n",
    "            chef_target,\n",
    "            random_state = 222,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = chef['FOLLOWED_RECOMMENDATIONS_PCT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Random Forrest </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.8286\n",
      "Testing  ACCURACY: 0.8172\n",
      "AUC Score        : 0.7481\n"
     ]
    }
   ],
   "source": [
    "rndfor = RandomForestClassifier(criterion = 'gini',\n",
    "                                bootstrap = True, \n",
    "                                max_depth = 4, \n",
    "                                n_estimators = 200,\n",
    "                                min_samples_leaf = 25, \n",
    "                                random_state = 222)\n",
    "\n",
    "# FITTING the training data\n",
    "rndfor_fit = rndfor.fit(X_train, y_train)\n",
    "\n",
    "# PREDICTING on test data\n",
    "rndfor_pred = rndfor_fit.predict(X_test)\n",
    "\n",
    "# SCORING the model\n",
    "print('Training ACCURACY:', rndfor_fit.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', rndfor_fit.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = rndfor_pred).round(4))\n",
    "\n",
    "model_performance.append(['Random Forrest w all var',\n",
    "                          rndfor_fit.score(X_train, y_train).round(4),\n",
    "                          rndfor_fit.score(X_test, y_test).round(4),\n",
    "                          roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = rndfor_pred).round(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> KNN </h3>\n",
    "\n",
    "<strong> Worst performing model in the whole analysis </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal number of neighbors is: 32\n",
      "Training ACCURACY: 0.6813\n",
      "Testing  ACCURACY: 0.7166\n",
      "AUC Score        : 0.5481\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a KNN classification model with optimal neighbors\n",
    "knn_opt = KNeighborsClassifier(n_neighbors = optimal_neighbors(X_train, \n",
    "                                                               y_train))\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "knn_fit = knn_opt.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "knn_pred = knn_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', knn_fit.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', knn_fit.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = knn_pred).round(4))\n",
    "\n",
    "model_performance.append(['KNeighbor w all var',\n",
    "                          knn_fit.score(X_train, y_train).round(4),\n",
    "                          knn_fit.score(X_test, y_test).round(4),\n",
    "                          roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = knn_pred).round(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Gradient Boost </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.9088\n",
      "Testing  ACCURACY: 0.7988\n",
      "AUC Score        : 0.7779\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a classification \n",
    "g_boost = GradientBoostingClassifier()\n",
    "\n",
    "# FITTING the training data\n",
    "g_boost_fit = g_boost.fit(X_train, y_train)\n",
    "\n",
    "# PREDICTING on test data\n",
    "g_boost_pred = g_boost_fit.predict(X_test)\n",
    "\n",
    "# SCORING the model\n",
    "print('Training ACCURACY:', g_boost_fit.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', g_boost_fit.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = g_boost_pred).round(4))\n",
    "\n",
    "\n",
    "model_performance.append(['GradientBoosting w all var',\n",
    "                          g_boost_fit.score(X_train, y_train).round(4),\n",
    "                          g_boost_fit.score(X_test, y_test).round(4),\n",
    "                          roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = g_boost_pred).round(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Model Summary </h2>\n",
    "\n",
    "\n",
    "This is a complete list of models and parameters used.\n",
    "\n",
    "The best model was <strong> Gradient Boost on significant variables with AUC score = 0.8181 </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Model</td>\n",
       "      <td>Training Accuracy</td>\n",
       "      <td>Testing Accuracy</td>\n",
       "      <td>AUC Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression w significant var</td>\n",
       "      <td>0.7807</td>\n",
       "      <td>0.7803</td>\n",
       "      <td>0.7505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Logistic Regression scaled significant var</td>\n",
       "      <td>0.7985</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>0.7057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Decision Tree w significant var</td>\n",
       "      <td>0.9596</td>\n",
       "      <td>0.7536</td>\n",
       "      <td>0.7377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Tuned Tree w significant var</td>\n",
       "      <td>0.8081</td>\n",
       "      <td>0.8111</td>\n",
       "      <td>0.7377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Decision tree w scaled var</td>\n",
       "      <td>0.9575</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>0.7142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Tuned Tree w scaled var</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.7844</td>\n",
       "      <td>0.492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>GradientBoosting w significant var</td>\n",
       "      <td>0.8328</td>\n",
       "      <td>0.8296</td>\n",
       "      <td>0.8181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Tuned GradientBoosting w significant var</td>\n",
       "      <td>0.8026</td>\n",
       "      <td>0.7906</td>\n",
       "      <td>0.7414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>GradientBoosting scaled w scaled var</td>\n",
       "      <td>0.8424</td>\n",
       "      <td>0.8172</td>\n",
       "      <td>0.7791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Random Forrest w significant var</td>\n",
       "      <td>0.8245</td>\n",
       "      <td>0.8111</td>\n",
       "      <td>0.7478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Tuned Random Forrest w significant var</td>\n",
       "      <td>0.8355</td>\n",
       "      <td>0.8193</td>\n",
       "      <td>0.7761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Random Forrest scaled w significant var</td>\n",
       "      <td>0.8259</td>\n",
       "      <td>0.7926</td>\n",
       "      <td>0.7254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>KNeighbor w significant var</td>\n",
       "      <td>0.8232</td>\n",
       "      <td>0.7988</td>\n",
       "      <td>0.7942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>KNeighbor Scaled w scaled var</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.7659</td>\n",
       "      <td>0.716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Random Forrest w all var</td>\n",
       "      <td>0.8286</td>\n",
       "      <td>0.8172</td>\n",
       "      <td>0.7481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>KNeighbor w all var</td>\n",
       "      <td>0.6813</td>\n",
       "      <td>0.7166</td>\n",
       "      <td>0.5481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>GradientBoosting w all var</td>\n",
       "      <td>0.9088</td>\n",
       "      <td>0.7988</td>\n",
       "      <td>0.7779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             0                  1  \\\n",
       "0                                        Model  Training Accuracy   \n",
       "1        Logistic Regression w significant var             0.7807   \n",
       "2   Logistic Regression scaled significant var             0.7985   \n",
       "3              Decision Tree w significant var             0.9596   \n",
       "4                 Tuned Tree w significant var             0.8081   \n",
       "5                   Decision tree w scaled var             0.9575   \n",
       "6                      Tuned Tree w scaled var              0.817   \n",
       "7           GradientBoosting w significant var             0.8328   \n",
       "8     Tuned GradientBoosting w significant var             0.8026   \n",
       "9         GradientBoosting scaled w scaled var             0.8424   \n",
       "10            Random Forrest w significant var             0.8245   \n",
       "11      Tuned Random Forrest w significant var             0.8355   \n",
       "12     Random Forrest scaled w significant var             0.8259   \n",
       "13                 KNeighbor w significant var             0.8232   \n",
       "14               KNeighbor Scaled w scaled var              0.804   \n",
       "15                    Random Forrest w all var             0.8286   \n",
       "16                         KNeighbor w all var             0.6813   \n",
       "17                  GradientBoosting w all var             0.9088   \n",
       "\n",
       "                   2          3  \n",
       "0   Testing Accuracy  AUC Value  \n",
       "1             0.7803     0.7505  \n",
       "2             0.7474     0.7057  \n",
       "3             0.7536     0.7377  \n",
       "4             0.8111     0.7377  \n",
       "5             0.7474     0.7142  \n",
       "6             0.7844      0.492  \n",
       "7             0.8296     0.8181  \n",
       "8             0.7906     0.7414  \n",
       "9             0.8172     0.7791  \n",
       "10            0.8111     0.7478  \n",
       "11            0.8193     0.7761  \n",
       "12            0.7926     0.7254  \n",
       "13            0.7988     0.7942  \n",
       "14            0.7659      0.716  \n",
       "15            0.8172     0.7481  \n",
       "16            0.7166     0.5481  \n",
       "17            0.7988     0.7779  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#taking model performance list and creating a dataframe\n",
    "model_performance = pd.DataFrame(model_performance)\n",
    "\n",
    "model_performance.round(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
